# -*- coding: utf-8 -*-
"""Twitter Bot Prototypes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xx649wiSCGq4Kp3avq2mwaQSaOFunj2U

This code and system were developed by Arsh Misra. Any commercial use or derivative work requires written permission.
"""

from openai import OpenAI

# Initialize OpenAI client with API key
client = OpenAI(api_key="")  # Replace with your API key

# Example post-reply pairs
previous_reply_pairs = [
    {"post": "Dudes reply â€œbangerâ€ to your tweet then steal it.", "reply": "Because itâ€™s a banger"},
    {"post": "So @grok didn't reply to our last post so check out the top projects on DappBay instead", "reply": "@grok why didnâ€™t you let them into your DMs?"},
    {"post": "You know the vibes", "reply": "Elite ball knowledge"},
    {"post": "WHY IS LITECOIN PUMPING AND MY COIN ISN'T!?!?", "reply": "2010 ahh meme. Brings me back"},
    {"post": "JUST RECEIVED VERY BULLISH NEWS", "reply": "Drop the alpha lil bro"},
    {"post": "How tax evasion looks at me after my first paycheck", "reply": "Uh oh"},
    {"post": "Hate to farm engagement, but really proud of this one... Finally reached 2,000 smart followers!", "reply": "Did I make it in?"},
    {"post": "What if ads paid you? Attention is becoming an asset class. Stay tuned ðŸ‘€", "reply": "YouTube better get on this"},
]

# Function to generate reply
def generate_reply(new_post, previous_reply_pairs=None, temperature=0.7):
    system_msg = "You are a helpful assistant that replies to social media posts in an engaging, intelligent, and thoughtful manner."

    if previous_reply_pairs:
        context = "Here are some example post-reply pairs:\n"
        for pair in previous_reply_pairs:
            context += f'Post: "{pair["post"]}"\nReply: "{pair["reply"]}"\n\n'
    else:
        context = "No previous post-reply data available.\n"

    user_msg = f"{context}Now write a thoughtful reply to this new post:\n\"{new_post}\""

    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            max_tokens=150,
            temperature=temperature
        )
        return response.choices[0].message.content

    except Exception as e:
        print(f"Error generating reply: {e}")
        return "Sorry, there was an error generating the reply."

#Prompt the user
user_prompt = input("Paste the post you want to reply to:\n")
reply = generate_reply(user_prompt, previous_reply_pairs)
print("\nGenerated Reply:\n", reply)

# Example post-reply pairs for few-shot learning
previous_reply_pairs = [
    {"post": "Dudes reply â€œbangerâ€ to your tweet then steal it.", "reply": "Because itâ€™s a banger"},
    {"post": "So @grok didn't reply to our last post so check out the top projects on DappBay instead", "reply": "@grok why didnâ€™t you let them into your DMs?"},
    {"post": "You know the vibes", "reply": "Elite ball knowledge"},
    {"post": "WHY IS LITECOIN PUMPING AND MY COIN ISN'T!?!?", "reply": "2010 ahh meme. Brings me back"},
    {"post": "JUST RECEIVED VERY BULLISH NEWS", "reply": "Drop the alpha lil bro"},
    {"post": "How tax evasion looks at me after my first paycheck", "reply": "Uh oh"},
    {"post": "Hate to farm engagement, but really proud of this one... Finally reached 2,000 smart followers!", "reply": "Did I make it in?"},
    {"post": "What if ads paid you? Attention is becoming an asset class. Stay tuned ðŸ‘€", "reply": "YouTube better get on this"},
]

# Function to generate CT-style reply with balanced creativity
def generate_reply(new_post, previous_reply_pairs=None, temperature=0.75):
    system_msg = (
        "You are a highly online crypto Twitter user. Reply with short, witty, and clever one-liners that feel natural and human. "
        "Your tone is often sharp and irreverent but can flex to be humorous, sarcastic, or lighthearted. "
        "Avoid sounding formal, robotic, or overly explanatory. Use crypto/meme culture references appropriately. "
        "Feel free to add new creative twists, but keep replies concise and on vibe."
    )

    if previous_reply_pairs:
        context = "Here are some example post-reply pairs:\n"
        for pair in previous_reply_pairs:
            context += f'Post: "{pair["post"]}"\nReply: "{pair["reply"]}"\n\n'
    else:
        context = "No previous post-reply data available.\n"

    user_msg = f"""{context}
Now write a reply to this new post in a similar style and tone, but donâ€™t copy the examples exactly.
Keep it under 20 words and avoid sounding like a bot or assistant.
"{new_post}"
Only output the reply."""

    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            max_tokens=60,
            temperature=temperature
        )

        return response.choices[0].message.content.strip()

    except Exception as e:
        print(f"Error generating reply: {e}")
        return "Sorry, there was an error generating the reply."

# Prompt user for new post
user_post = input("Paste a new post to reply to:\n")

# Generate and print the reply
reply = generate_reply(user_post, previous_reply_pairs)
print("\nGenerated Reply:\n", reply)

import tweepy

# Set up client
client = tweepy.Client(bearer_token="", consumer_key= "", consumer_secret="", access_token="", access_token_secret="")

target_users = ["gemini", "0xwenmoon", "sing_me_a"]

for username in target_users:
    user = client.get_user(username=username)
    tweets = client.get_users_tweets(id=user.data.id, max_results=5)
    for tweet in tweets.data:
        # Check if we already replied or processed tweet.id
        if not already_replied(tweet.id):
            reply_text = generate_reply(tweet.text, previous_reply_pairs)
            client.create_tweet(text=reply_text, in_reply_to_tweet_id=tweet.id)
            log_reply(tweet.id, reply_text)

import time
from openai import OpenAI
import tweepy
from tweepy.errors import TooManyRequests

previous_reply_pairs = [
    {"post": "Dudes reply â€œbangerâ€ to your tweet then steal it.", "reply": "Because itâ€™s a banger"},
    {"post": "So @grok didn't reply to our last post so check out the top projects on DappBay instead", "reply": "@grok why didnâ€™t you let them into your DMs?"},
    {"post": "You know the vibes", "reply": "Elite ball knowledge"},
    {"post": "WHY IS LITECOIN PUMPING AND MY COIN ISN'T!?!?", "reply": "2010 ahh meme. Brings me back"},
    {"post": "JUST RECEIVED VERY BULLISH NEWS", "reply": "Drop the alpha lil bro"},
    {"post": "How tax evasion looks at me after my first paycheck", "reply": "Uh oh"},
    {"post": "Hate to farm engagement, but really proud of this one... Finally reached 2,000 smart followers!", "reply": "Did I make it in?"},
    {"post": "What if ads paid you? Attention is becoming an asset class. Stay tuned ðŸ‘€", "reply": "YouTube better get on this"},
]

def pick_style_based_on_post(post):
    serious_keywords = ["regulation", "law", "security", "audit", "risk", "scam", "legal", "warning", "investor", "fund"]
    hype_keywords = ["moon", "bull", "pump", "all time high", "breaking", "bullish", "to the moon"]
    sarcastic_keywords = ["lol", "wtf", "smh", "lmao", "fail", "rip", "dead"]

    post_lower = post.lower()
    if any(word in post_lower for word in serious_keywords):
        return "serious"
    elif any(word in post_lower for word in hype_keywords):
        return "hype"
    elif any(word in post_lower for word in sarcastic_keywords):
        return "sarcastic"
    else:
        return "default"

style_prompts = {
    "default": "short, witty, and clever one-liners that feel natural and human.",
    "sarcastic": "dry, sarcastic, and sharp one-liners with a biting tone.",
    "hype": "energetic, hype, and enthusiastic short replies.",
    "serious": "thoughtful, professional, and informative replies suitable for serious posts.",
    "dry": "minimalist, deadpan, and dry humor one-liners."
}

def generate_reply(new_post, previous_reply_pairs=None, style=None, temperature=0.75, max_tokens=60):
    if style is None:
        style = pick_style_based_on_post(new_post)

    tone_instruction = style_prompts.get(style, style_prompts["default"])

    system_msg = (
        f"You are a highly online crypto Twitter user. Reply with {tone_instruction} "
        "Avoid sounding formal, robotic, or overly explanatory. Use crypto/meme culture references appropriately. "
        "Feel free to add new creative twists, but keep replies concise and on vibe."
    )

    if previous_reply_pairs:
        context = "Here are some example post-reply pairs:\n"
        for pair in previous_reply_pairs:
            context += f'Post: "{pair["post"]}"\nReply: "{pair["reply"]}"\n\n'
    else:
        context = "No previous post-reply data available.\n"

    user_msg = f"""{context}
Now write a reply to this new post in a similar style and tone, but donâ€™t copy the examples exactly.
Keep it under 20 words and avoid sounding like a bot or assistant.
{new_post}
Only output the reply."""

    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            max_tokens=max_tokens,
            temperature=temperature
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        print(f"Error generating reply: {e}")
        return "Sorry, there was an error generating the reply."


def wait_if_rate_limited(exception):
    reset_timestamp = int(exception.response.headers.get("x-rate-limit-reset", time.time() + 60))
    wait_seconds = max(reset_timestamp - time.time(), 0) + 1
    print(f"Rate limit hit! Waiting for {int(wait_seconds)} seconds...")
    time.sleep(wait_seconds)


# ... all your function definitions here ...

replied_ids = set()  # or load from persistence if you want

if __name__ == "__main__":
    account_label = input("Target account label (or just type a name): ").strip()
    post_text, reply_to_tweet_id = get_latest_tweet(account_label)  # pass any needed params

    if post_text is None or reply_to_tweet_id is None:
        print(f"Could not fetch latest tweet from @{account_label}. Exiting.")
        exit()

    print(f"Fetched latest tweet from @{account_label}:\n{post_text}\n")

    # continue with your existing enrichment, classification, reply generation...


def main():
    target_user = "bitgetglobal"

    try:
        user_response = twitter_client.get_user(username=target_user)
        user_id = user_response.data.id

        tweets_response = twitter_client.get_users_tweets(
            id=user_id,
            max_results=5,
            tweet_fields=["created_at", "referenced_tweets"]
        )
        tweets = tweets_response.data if tweets_response.data else []

        for tweet in tweets:
            if tweet.id in replied_ids:
                print("Already replied to this tweet. Skipping.")
                continue

            referenced = getattr(tweet, "referenced_tweets", None)
            if referenced and any(ref.type in ["retweeted", "replied_to"] for ref in referenced):
                print(f"Skipping retweet or reply: {tweet.id}")
                continue

            print(f"Tweet: {tweet.text}")
            reply_text = generate_reply(tweet.text, previous_reply_pairs)
            reply_text = reply_text.strip().strip('"â€œâ€')  # âœ… Strip wrapping quotes
            print(f"Reply: {reply_text}")

            twitter_client.create_tweet(text=reply_text, in_reply_to_tweet_id=tweet.id)
            print(f"Replied to tweet ID: {tweet.id}")
            replied_ids.add(tweet.id)
            break  # only reply to the most recent eligible tweet

    except TooManyRequests as e:
        wait_if_rate_limited(e)
    except Exception as e:
        print(f"Unexpected error: {e}")

if __name__ == "__main__":
    main()

# Colab prototype: algorithm-first reply generator (manual input)
# ---------------------------------------------------------------
# Installs (first cell)
#!pip install --quiet openai sentence-transformers transformers torch requests beautifulsoup4 pandas openpyxl

# ---------------------------------------------------------------
# Imports & client init
#from openai import OpenAI
import os, time, re, requests, pandas as pd, numpy as np, datetime
from bs4 import BeautifulSoup

from sentence_transformers import SentenceTransformer
import numpy as np

# Model to call (change to "gpt-4o" if you have access / prefer)
MODEL = "gpt-4"

# ---------------------------------------------------------------
# Example training data fallback (you can replace this with an Excel/CSV)
previous_reply_pairs = [
    {"post": "Dudes reply â€œbangerâ€ to your tweet then steal it.", "reply": "Because itâ€™s a banger"},
    {"post": "So @grok didn't reply to our last post so check out the top projects on DappBay instead", "reply": "@grok why didnâ€™t you let them into your DMs?"},
    {"post": "You know the vibes", "reply": "Elite ball knowledge"},
    {"post": "WHY IS LITECOIN PUMPING AND MY COIN ISN'T!?!?", "reply": "2010 ahh meme. Brings me back"},
    {"post": "JUST RECEIVED VERY BULLISH NEWS", "reply": "Drop the alpha lil bro"},
    {"post": "How tax evasion looks at me after my first paycheck", "reply": "Uh oh"},
    {"post": "What if ads paid you? Attention is becoming an asset class. Stay tuned ðŸ‘€", "reply": "YouTube better get on this"},

    # Added new raw text examples:
    {"post": "Opening CT today, we are officially in a bullrun", "reply": "We da awakest"},
    {"post": "Overheard:", "reply": "Arbitrum Autumn Next"},
    {"post": "â€œBeautiful chart or hot OF?â€ â€œYes.â€", "reply": "just pure psychology"},
    {"post": "weâ€™re looking at the data and some of it blows my mind", "reply": "ETHereal"},
    {"post": "Order flow segmentation is a cornerstone of TradFi infrastructure, but many constraints have made it tough to implement in DeFi... until now.", "reply": "Because itâ€™s a banger"},
    {"post": "At aPriori, we make users a priority. Thatâ€™s why we built Swapr, and this is just a teaser of whatâ€™s coming.", "reply": "Drop the alpha lil bro"},
    {"post": "Taking off from an Asian airport and landing in burger land breaks my heart every time.", "reply": "Uh oh"},
    {"post": "I FUCKING KNEW IT â˜ ï¸â˜ ï¸â˜ ï¸â˜ ï¸â˜ ï¸â˜ ï¸", "reply": "We da awakest"},
    {"post": "Arbitrum August", "reply": "Arbitrum Autumn Next"},
    {"post": "I'm rawdogging these charts No TA, no lines, no fibs. Just pure vibe trading", "reply": "just pure psychology"},
    {"post": "Believe in somETHing.", "reply": "ETHereal"},
]


# ---------------------------------------------------------------
# Embedding model init
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

# Example types & classification function
example_types = {
    "education": [
        "How to build a neural network from scratch",
        "Tutorial: understanding transformers in NLP",
        "Here's a detailed guide on smart contract security",
        "Explaining differential privacy in data science"
    ],
    "meme": [
        "You know the vibes",
        "Why is my coin not pumping?!",
        "Just received very bullish news",
        "When the market dips but you hodl anyway"
    ],
    "shill": [
        "Check out this amazing new project!",
        "Buy $XYZ token before it moons!",
        "Don't miss this alpha drop ðŸš€",
        "Join our Discord for exclusive updates!"
    ]
}

def classify_post_type(new_post):
    new_emb = embed_model.encode([new_post], convert_to_numpy=True)[0]
    scores = {}
    for t, examples in example_types.items():
        example_embs = embed_model.encode(examples, convert_to_numpy=True)
        sims = [float(np.dot(new_emb, e) / (np.linalg.norm(new_emb)*np.linalg.norm(e))) for e in example_embs]
        scores[t] = max(sims)  # similarity to most similar example
    return max(scores, key=scores.get)

# ---------------------------------------------------------------
# Helpers: load examples from CSV/XLSX if available
def load_examples(path="examples.xlsx"):
    if os.path.exists(path):
        if path.lower().endswith(".xlsx") or path.lower().endswith(".xls"):
            df = pd.read_excel(path)
        else:
            df = pd.read_csv(path)
        # ensure 'post' and 'reply' columns exist
        if "post" not in df.columns or "reply" not in df.columns:
            raise ValueError("examples file must have 'post' and 'reply' columns")
        return df.reset_index(drop=True)
    else:
        return pd.DataFrame(previous_reply_pairs)

examples_df = load_examples("examples.xlsx")  # if file absent, falls back to previous_reply_pairs

def ensure_example_embeddings(df):
    if df.empty:
        return df
    if "embed" not in df.columns:
        posts = df['post'].astype(str).tolist()
        emb = embed_model.encode(posts, convert_to_numpy=True)
        df = df.copy()
        df['embed'] = list(emb)  # store numpy arrays as list elements
    return df

examples_df = ensure_example_embeddings(examples_df)

def get_top_k_examples(new_post, df=examples_df, k=4):
    if df.empty:
        return []
    new_emb = embed_model.encode([new_post], convert_to_numpy=True)[0]
    sims = []
    for e in df['embed'].tolist():
        a = np.array(e)
        sims.append(float(np.dot(new_emb, a) / (np.linalg.norm(new_emb) * np.linalg.norm(a) + 1e-10)))
    idx = np.argsort(sims)[::-1][:k]
    return df.iloc[idx].to_dict('records')

# ---------------------------------------------------------------
# Enrichment: link summarization (best-effort), image captioning (best-effort), comments sentiment (fallback)
def summarize_url(url, max_chars=4000):
    try:
        resp = requests.get(url, timeout=8, headers={"User-Agent":"Mozilla/5.0"})
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        paragraphs = [p.get_text().strip() for p in soup.find_all("p")]
        text = " ".join(paragraphs)
        text = text[:max_chars]
        if not text.strip():
            return None
        prompt = f"Summarize the following article in 3 concise bullets (one sentence each):\n\n{text}"
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role":"system","content":"You are a concise summarizer."},
                {"role":"user","content":prompt}
            ],
            max_tokens=200,
            temperature=0
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return None

def caption_image(image_url):
    try:
        from transformers import pipeline
        cap = pipeline("image-captioning", model="Salesforce/blip-image-captioning-base")
        out = cap(image_url, max_length=40)
        if isinstance(out, list) and len(out) > 0:
            first = out[0]
            if isinstance(first, dict):
                return first.get("generated_text") or first.get("caption") or str(first)
            return str(first)
        return None
    except Exception as e:
        return None

def analyze_comments_emotions(comments_list):
    if not comments_list:
        return {"summary":"no_comments","counts":{}}
    try:
        from transformers import pipeline
        emotion_classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=True)
        results = emotion_classifier(comments_list)
        counts = {}
        for res in results:
            top_emotion = max(res, key=lambda x: x['score'])['label']
            counts[top_emotion] = counts.get(top_emotion, 0) + 1
        summary = ", ".join(f"{k}:{v}" for k,v in counts.items())
        return {"summary":summary, "counts":counts}
    except Exception as e:
        return {"summary":"emotion_analysis_failed","counts":{}}

# ---------------------------------------------------------------
# Prompt assembler
def build_generation_prompt(new_post, top_examples=None, image_caption=None, link_summary=None, comment_summary=None, account_label=None, post_type=None):
    system_prompt = (
        "You are a concise, witty, human-sounding social media reply writer. "
        "Avoid robotic phrasing and verbosity. "
        "Match the tone of the original post and the sentiment of the comments carefully. "
        "If comments sentiment is NEGATIVE or critical, reply empathetically and briefly, acknowledging concerns but do not over-explain or be overly positive. "
        "If comments sentiment is NEUTRAL, reply concisely with a balanced tone. "
        "If comments sentiment is POSITIVE, keep the reply upbeat, energetic, and engaging. "
        "Always keep replies SHORT â€” no more than 20 words. Do not include hashtags or URLs."
    )

    # Determine sentiment label simply:
    sentiment_label = "neutral"
    if comment_summary:
        summary_lower = comment_summary.lower()
        if any(w in summary_lower for w in ["negative", "anger", "sad", "fear", "disgust", "critical"]):
            sentiment_label = "negative"
        elif any(w in summary_lower for w in ["positive", "joy", "happy", "excited"]):
            sentiment_label = "positive"
        else:
            sentiment_label = "neutral"

    context = f"Original post:\n{new_post}\n\n"
    context += f"Comments sentiment label: {sentiment_label}\n\n"

    # Style/length notes by post type
    if post_type == "education":
        style = "detailed and informative"
        length_note = "Aim for a thorough, educational reply with some depth but still concise."
    elif post_type == "meme":
        style = "short, casual, and witty"
        length_note = "Keep replies brief and light, max 8 words."
    elif post_type == "shill":
        style = "energetic and hype"
        length_note = "Use short and punchy replies, max 10 words."
    else:
        style = "concise and engaging"
        length_note = "Keep replies natural, balanced, and under 20 words."

    context += f"\nStyle: {style}\n{length_note}\n"
    context += "Generate a reply matching the sentiment and style above."

    if account_label:
        context += f" Target account: {account_label}\n\n"
    if top_examples:
        context += "Example post â‡¢ reply pairs (keep style consistent):\n"
        for ex in top_examples:
            p = ex.get("post","").replace("\n"," ")
            r = ex.get("reply","").replace("\n"," ")
            context += f'Post: "{p}"\nReply: "{r}"\n\n'
    context += f'New post: "{new_post}"\n\n'
    if image_caption:
        context += f"Image summary: {image_caption}\n"
    if link_summary:
        context += f"Link summary: {link_summary}\n"
    if comment_summary:
        context += f"Full comment sentiment summary: {comment_summary}\n"

    context += "\nReply must be SHORT (max 20 words), match the sentiment and style. Do NOT include hashtags, URLs, or emojis unless part of style.\n"

    return system_prompt, context

# ---------------------------------------------------------------
# Candidate generation & scoring
def generate_candidate_reply(system_prompt, user_content, temperature=0.7, max_tokens=80):
    try:
        r = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role":"system","content":system_prompt},
                {"role":"user","content":user_content}
            ],
            max_tokens=max_tokens,
            temperature=temperature
        )
        return r.choices[0].message.content.strip()
    except Exception as e:
        return f"(error generating candidate: {e})"

def score_candidate_with_model(post_context, candidate):
    prompt = f"Given the post:\n{post_context}\n\nAnd this reply candidate:\n{candidate}\n\nOn a scale 0-100, how likely is this reply to get above-average engagement (likes/retweets/replies) compared to a neutral reply on this topic? Answer with a single integer score followed by one short sentence justification."
    try:
        r = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role":"system","content":"You are an objective social-media engagement estimator."},
                {"role":"user","content":prompt}
            ],
            max_tokens=60,
            temperature=0
        )
        out = r.choices[0].message.content.strip()
        m = re.search(r"(\d{1,3})", out)
        score = int(m.group(1)) if m else None
        return score, out
    except Exception as e:
        return None, f"scoring_error: {e}"

# ---------------------------------------------------------------
# Run interactive flow (manual input)
account_label = input("Target account label (or just type a name): ").strip()
post_text = input("Paste the post text: ").strip()
image_url = input("Optional - image URL (or leave blank): ").strip() or None
link_url = input("Optional - link URL (or leave blank): ").strip() or None
comments_raw = input("Optional - paste top comments separated by ' || ' (or leave blank): ").strip()
comments_list = [c.strip() for c in comments_raw.split("||")] if comments_raw else []

# Classify post type AFTER getting post_text input
post_type = classify_post_type(post_text)
print(f"Classified post type as: {post_type}")

print("\nEnriching context (best-effort â€” may take a few seconds)...")
top_examples = get_top_k_examples(post_text, k=4)
image_caption = caption_image(image_url) if image_url else None
link_summary = summarize_url(link_url) if link_url else None
comment_summary = analyze_comments_emotions(comments_list)

system_prompt, user_content = build_generation_prompt(
    new_post=post_text,
    top_examples=top_examples,
    image_caption=image_caption,
    link_summary=link_summary,
    comment_summary=comment_summary.get("summary") if isinstance(comment_summary, dict) else comment_summary,
    account_label=account_label,
    post_type=post_type  # Pass post_type here
)

# Generate multiple candidates (different temps)
max_tokens_map = {
    "education": 60,
    "meme": 20,
    "shill": 20,
    "default": 40
}

post_type = classify_post_type(post_text)
print(f"Classified post type as: {post_type}")

max_tokens = max_tokens_map.get(post_type, max_tokens_map["default"])

temps = [0.2, 0.6, 1.0]
candidates = []
for t in temps:
    c = generate_candidate_reply(system_prompt, user_content, temperature=t, max_tokens=max_tokens)
    candidates.append({"temperature": t, "text": c})


# Score each candidate
for c in candidates:
    sc, justification = score_candidate_with_model(post_text, c["text"])
    c["score"] = sc
    c["justification"] = justification

# Present candidates sorted by score (if scoring succeeded)
candidates_sorted = sorted(candidates, key=lambda x: (x["score"] is not None, x["score"] if x["score"] is not None else -999), reverse=True)

import pandas as pd
df_out = pd.DataFrame(candidates_sorted)
print("\nCandidates (sorted by estimated score):\n")
display(df_out[["temperature","score","text","justification"]])

# ---------------------------------------------------------------
# Persist to log for later analysis
log_row = []
ts = datetime.datetime.utcnow().isoformat()
for c in candidates_sorted:
    log_row.append({
        "timestamp":ts,
        "account_label":account_label,
        "post_text":post_text,
        "image_caption":image_caption,
        "link_summary":link_summary,
        "comment_summary":comment_summary.get("summary") if isinstance(comment_summary, dict) else comment_summary,
        "candidate_text":c["text"],
        "candidate_temp":c["temperature"],
        "candidate_score":c.get("score"),
        "candidate_justification":c.get("justification")
    })
log_df = pd.DataFrame(log_row)
log_path = "reply_generation_log.csv"
if os.path.exists(log_path):
    existing = pd.read_csv(log_path)
    combined = pd.concat([existing, log_df], ignore_index=True)
    combined.to_csv(log_path, index=False)
else:
    log_df.to_csv(log_path, index=False)

print(f"\nLogged {len(log_row)} candidates to {log_path}.")
print("Done â€” pick a reply, iterate, and we can wire up automated scraping & training next.")

# Colab prototype: algorithm-first reply generator (manual input)
# ---------------------------------------------------------------
# Installs (first cell)
!pip install --quiet openai sentence-transformers transformers torch requests beautifulsoup4 pandas openpyxl tweepy

# ---------------------------------------------------------------

import os, time, re, requests, pandas as pd, numpy as np, datetime
from bs4 import BeautifulSoup

from sentence_transformers import SentenceTransformer
import numpy as np
import tweepy  # Added tweepy import

# Put your key here or set env var OPENAI_API_KEY

# Model to call (change to "gpt-4o" if you have access / prefer)
MODEL = "gpt-4"

# ---------------------------------------------------------------
# Twitter API credentials (fill in or use env vars)
# Tweepy client for posting tweets (needs OAuth1 for posting)
auth = tweepy.OAuth1UserHandler(
    TWITTER_API_KEY, TWITTER_API_SECRET,
    TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_SECRET
)
twitter_api = tweepy.API(auth)

# Tweepy Client for v2 read-only endpoints (fetching tweets, user info)
twitter_client = tweepy.Client(
    bearer_token=TWITTER_BEARER_TOKEN,
    consumer_key=TWITTER_API_KEY,
    consumer_secret=TWITTER_API_SECRET,
    access_token=TWITTER_ACCESS_TOKEN,
    access_token_secret=TWITTER_ACCESS_SECRET,
    wait_on_rate_limit=True
)

def post_reply(text, reply_to_tweet_id):
    try:
        response = twitter_api.update_status(
            status=text,
            in_reply_to_status_id=reply_to_tweet_id,
            auto_populate_reply_metadata=True
        )
        return response
    except Exception as e:
        print(f"Error posting reply: {e}")
        return None

def get_latest_tweet(account_handle, twitter_client=twitter_client, replied_ids=None):
    try:
        # Get user ID from username
        user_response = twitter_client.get_user(username=account_handle)
        user_id = user_response.data.id

        # Fetch user tweets (exclude retweets and replies)
        tweets_response = twitter_client.get_users_tweets(
            id=user_id,
            max_results=10,
            tweet_fields=["created_at", "referenced_tweets"]
        )
        tweets = tweets_response.data if tweets_response.data else []

        # Filter out retweets and replies
        for tweet in tweets:
            if replied_ids and tweet.id in replied_ids:
                continue
            referenced = getattr(tweet, "referenced_tweets", None)
            if referenced and any(ref.type in ["retweeted", "replied_to"] for ref in referenced):
                continue
            return tweet.text, tweet.id

        print(f"No suitable tweets found for @{account_handle}")
        return None, None

    except tweepy.TooManyRequests as e:
        print("Rate limit hit, waiting 15 minutes...")
        time.sleep(15 * 60)
        return None, None
    except Exception as e:
        print(f"Error fetching latest tweet: {e}")
        return None, None


# ---------------------------------------------------------------
# Example training data fallback (you can replace this with an Excel/CSV)
previous_reply_pairs = [
    {"post": "Dudes reply â€œbangerâ€ to your tweet then steal it.", "reply": "Because itâ€™s a banger"},
    {"post": "So @grok didn't reply to our last post so check out the top projects on DappBay instead", "reply": "@grok why didnâ€™t you let them into your DMs?"},
    {"post": "You know the vibes", "reply": "Elite ball knowledge"},
    {"post": "WHY IS LITECOIN PUMPING AND MY COIN ISN'T!?!?", "reply": "2010 ahh meme. Brings me back"},
    {"post": "JUST RECEIVED VERY BULLISH NEWS", "reply": "Drop the alpha lil bro"},
    {"post": "How tax evasion looks at me after my first paycheck", "reply": "Uh oh"},
    {"post": "What if ads paid you? Attention is becoming an asset class. Stay tuned ðŸ‘€", "reply": "YouTube better get on this"},
    # Added new raw text examples:
    {"post": "Opening CT today, we are officially in a bullrun", "reply": "We da awakest"},
    {"post": "Overheard:", "reply": "Arbitrum Autumn Next"},
    {"post": "â€œBeautiful chart or hot OF?â€ â€œYes.â€", "reply": "just pure psychology"},
    {"post": "weâ€™re looking at the data and some of it blows my mind", "reply": "ETHereal"},
    {"post": "Order flow segmentation is a cornerstone of TradFi infrastructure, but many constraints have made it tough to implement in DeFi... until now.", "reply": "Because itâ€™s a banger"},
    {"post": "At aPriori, we make users a priority. Thatâ€™s why we built Swapr, and this is just a teaser of whatâ€™s coming.", "reply": "Drop the alpha lil bro"},
    {"post": "Taking off from an Asian airport and landing in burger land breaks my heart every time.", "reply": "Uh oh"},
    {"post": "I FUCKING KNEW IT â˜ ï¸â˜ ï¸â˜ ï¸â˜ ï¸â˜ ï¸â˜ ï¸", "reply": "We da awakest"},
    {"post": "Arbitrum August", "reply": "Arbitrum Autumn Next"},
    {"post": "I'm rawdogging these charts No TA, no lines, no fibs. Just pure vibe trading", "reply": "just pure psychology"},
    {"post": "Believe in somETHing.", "reply": "ETHereal"},
]

# ---------------------------------------------------------------
# Embedding model init
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

# Example types & classification function
example_types = {
    "education": [
        "How to build a neural network from scratch",
        "Tutorial: understanding transformers in NLP",
        "Here's a detailed guide on smart contract security",
        "Explaining differential privacy in data science"
    ],
    "meme": [
        "You know the vibes",
        "Why is my coin not pumping?!",
        "Just received very bullish news",
        "When the market dips but you hodl anyway"
    ],
    "shill": [
        "Check out this amazing new project!",
        "Buy $XYZ token before it moons!",
        "Don't miss this alpha drop ðŸš€",
        "Join our Discord for exclusive updates!"
    ]
}

def classify_post_type(new_post):
    new_emb = embed_model.encode([new_post], convert_to_numpy=True)[0]
    scores = {}
    for t, examples in example_types.items():
        example_embs = embed_model.encode(examples, convert_to_numpy=True)
        sims = [float(np.dot(new_emb, e) / (np.linalg.norm(new_emb)*np.linalg.norm(e))) for e in example_embs]
        scores[t] = max(sims)  # similarity to most similar example
    return max(scores, key=scores.get)

# ---------------------------------------------------------------
# Helpers: load examples from CSV/XLSX if available
def load_examples(path="examples.xlsx"):
    if os.path.exists(path):
        if path.lower().endswith(".xlsx") or path.lower().endswith(".xls"):
            df = pd.read_excel(path)
        else:
            df = pd.read_csv(path)
        # ensure 'post' and 'reply' columns exist
        if "post" not in df.columns or "reply" not in df.columns:
            raise ValueError("examples file must have 'post' and 'reply' columns")
        return df.reset_index(drop=True)
    else:
        return pd.DataFrame(previous_reply_pairs)

examples_df = load_examples("examples.xlsx")  # if file absent, falls back to previous_reply_pairs

def ensure_example_embeddings(df):
    if df.empty:
        return df
    if "embed" not in df.columns:
        posts = df['post'].astype(str).tolist()
        emb = embed_model.encode(posts, convert_to_numpy=True)
        df = df.copy()
        df['embed'] = list(emb)  # store numpy arrays as list elements
    return df

examples_df = ensure_example_embeddings(examples_df)

def get_top_k_examples(new_post, df=examples_df, k=4):
    if df.empty:
        return []
    new_emb = embed_model.encode([new_post], convert_to_numpy=True)[0]
    sims = []
    for e in df['embed'].tolist():
        a = np.array(e)
        sims.append(float(np.dot(new_emb, a) / (np.linalg.norm(new_emb) * np.linalg.norm(a) + 1e-10)))
    idx = np.argsort(sims)[::-1][:k]
    return df.iloc[idx].to_dict('records')

# ---------------------------------------------------------------
# Enrichment: link summarization (best-effort), image captioning (best-effort), comments sentiment (fallback)
def summarize_url(url, max_chars=4000):
    try:
        resp = requests.get(url, timeout=8, headers={"User-Agent":"Mozilla/5.0"})
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        paragraphs = [p.get_text().strip() for p in soup.find_all("p")]
        text = " ".join(paragraphs)
        text = text[:max_chars]
        if not text.strip():
            return None
        prompt = f"Summarize the following article in 3 concise bullets (one sentence each):\n\n{text}"
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role":"system","content":"You are a concise summarizer."},
                {"role":"user","content":prompt}
            ],
            max_tokens=200,
            temperature=0
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return None

def caption_image(image_url):
    try:
        from transformers import pipeline
        cap = pipeline("image-captioning", model="Salesforce/blip-image-captioning-base")
        out = cap(image_url, max_length=40)
        if isinstance(out, list) and len(out) > 0:
            first = out[0]
            if isinstance(first, dict):
                return first.get("generated_text") or first.get("caption") or str(first)
            return str(first)
        return None
    except Exception as e:
        return None

def analyze_comments_emotions(comments_list):
    if not comments_list:
        return {"summary":"no_comments","counts":{}}
    try:
        from transformers import pipeline
        emotion_classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", return_all_scores=True)
        results = emotion_classifier(comments_list)
        counts = {}
        for res in results:
            top_emotion = max(res, key=lambda x: x['score'])['label']
            counts[top_emotion] = counts.get(top_emotion, 0) + 1
        summary = ", ".join(f"{k}:{v}" for k,v in counts.items())
        return {"summary":summary, "counts":counts}
    except Exception as e:
        return {"summary":"emotion_analysis_failed","counts":{}}

# ---------------------------------------------------------------
# Prompt assembler
def build_generation_prompt(new_post, top_examples=None, image_caption=None, link_summary=None, comment_summary=None, account_label=None, post_type=None):
    system_prompt = (
        "You are a concise, witty, human-sounding social media reply writer. "
        "Avoid robotic phrasing and verbosity. "
        "Match the tone of the original post and the sentiment of the comments carefully. "
        "If comments sentiment is NEGATIVE or critical, reply empathetically and briefly, acknowledging concerns but do not over-explain or be overly positive. "
        "If comments sentiment is NEUTRAL, reply concisely with a balanced tone. "
        "If comments sentiment is POSITIVE, keep the reply upbeat, energetic, and engaging. "
        "Always keep replies SHORT â€” no more than 20 words. Do not include hashtags or URLs."
    )

    # Determine sentiment label simply:
    sentiment_label = "neutral"
    if comment_summary:
        summary_lower = comment_summary.lower()
        if any(w in summary_lower for w in ["negative", "anger", "sad", "fear", "disgust", "critical"]):
            sentiment_label = "negative"
        elif any(w in summary_lower for w in ["positive", "joy", "happy", "excited"]):
            sentiment_label = "positive"
        else:
            sentiment_label = "neutral"

    context = f"Original post:\n{new_post}\n\n"
    context += f"Comments sentiment label: {sentiment_label}\n\n"

    # Style/length notes by post type
    if post_type == "education":
        style = "detailed and informative"
        length_note = "Aim for a thorough, educational reply with some depth but still concise."
    elif post_type == "meme":
        style = "short, casual, and witty"
        length_note = "Keep replies brief and light, max 8 words."
    elif post_type == "shill":
        style = "energetic and hype"
        length_note = "Use short and punchy replies, max 10 words."
    else:
        style = "concise and engaging"
        length_note = "Keep replies natural, balanced, and under 20 words."

    context += f"\nStyle: {style}\n{length_note}\n"
    context += "Generate a reply matching the sentiment and style above."

    if account_label:
        context += f" Target account: {account_label}\n\n"
    if top_examples:
        context += "Example post â‡¢ reply pairs (keep style consistent):\n"
        for ex in top_examples:
            p = ex.get("post","").replace("\n"," ")
            r = ex.get("reply","").replace("\n"," ")
            context += f'Post: "{p}"\nReply: "{r}"\n\n'
    context += f'New post: "{new_post}"\n\n'
    if image_caption:
        context += f"Image summary: {image_caption}\n"
    if link_summary:
        context += f"Link summary: {link_summary}\n"
    if comment_summary:
        context += f"Full comment sentiment summary: {comment_summary}\n"

    context += "\nReply must be SHORT (max 20 words), match the sentiment and style. Do NOT include hashtags, URLs, or emojis unless part of style.\n"

    return system_prompt, context

# ---------------------------------------------------------------
# Candidate generation & scoring
def generate_candidate_reply(system_prompt, user_content, temperature=0.7, max_tokens=80):
    try:
        r = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role":"system","content":system_prompt},
                {"role":"user","content":user_content}
            ],
            max_tokens=max_tokens,
            temperature=temperature
        )
        return r.choices[0].message.content.strip()
    except Exception as e:
        return f"(error generating candidate: {e})"

def score_candidate_with_model(post_context, candidate):
    prompt = f"Given the post:\n{post_context}\n\nAnd this reply candidate:\n{candidate}\n\nOn a scale 0-100, how likely is this reply to get above-average engagement (likes/retweets/replies) compared to a neutral reply on this topic? Answer with a single integer score followed by one short sentence justification."
    try:
        r = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role":"system","content":"You are an objective social-media engagement estimator."},
                {"role":"user","content":prompt}
            ],
            max_tokens=60,
            temperature=0
        )
        out = r.choices[0].message.content.strip()
        m = re.search(r"(\d{1,3})", out)
        score = int(m.group(1)) if m else None
        return score, out
    except Exception as e:
        return None, f"scoring_error: {e}"

# ---------------------------------------------------------------
# Run interactive flow (manual input)
account_label = input("Target account label (or just type a name): ").strip()

# Fetch latest tweet text and id automatically:
post_text, reply_to_tweet_id = get_latest_tweet(account_label)
if post_text is None or reply_to_tweet_id is None:
    print(f"Could not fetch latest tweet from @{account_label}. Exiting.")
    exit()

print(f"Fetched latest tweet from @{account_label}:\n{post_text}\n")

image_url = input("Optional - image URL (or leave blank): ").strip() or None
link_url = input("Optional - link URL (or leave blank): ").strip() or None
comments_raw = input("Optional - paste top comments separated by ' || ' (or leave blank): ").strip()
comments_list = [c.strip() for c in comments_raw.split("||")] if comments_raw else []

# Classify post type AFTER fetching post_text
post_type = classify_post_type(post_text)
print(f"Classified post type as: {post_type}")

print("\nEnriching context (best-effort â€” may take a few seconds)...")
top_examples = get_top_k_examples(post_text, k=4)
image_caption = caption_image(image_url) if image_url else None
link_summary = summarize_url(link_url) if link_url else None
comment_summary = analyze_comments_emotions(comments_list) if comments_list else None
comment_summary_text = comment_summary.get("summary") if comment_summary else None

system_prompt, user_content = build_generation_prompt(
    new_post=post_text,
    top_examples=top_examples,
    image_caption=image_caption,
    link_summary=link_summary,
    comment_summary=comment_summary_text,
    account_label=account_label,
    post_type=post_type
)

print("\nPrompting model to generate candidate replies...")
candidate = generate_candidate_reply(system_prompt, user_content)
print(f"\nCandidate reply:\n{candidate}\n")

# Optional: score candidate
score, rationale = score_candidate_with_model(post_text, candidate)
if score is not None:
    print(f"Estimated engagement score: {score}/100\nRationale: {rationale}")
else:
    print(f"Could not score candidate reply: {rationale}")

# Confirm posting (manual)
should_post = input("\nPost this reply to Twitter? (y/N): ").strip().lower()
if should_post == "y":
    post_response = post_reply(candidate, reply_to_tweet_id)
    if post_response:
        print(f"Reply posted successfully: https://twitter.com/{account_label}/status/{post_response.id}")
    else:
        print("Failed to post reply.")
else:
    print("Reply not posted. Exiting.")